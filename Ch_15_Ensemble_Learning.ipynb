{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch-15-Ensemble-Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVP3ayT4jXvNzyHqggzkbz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekchandrajha/mastering-ml-algorithms/blob/main/Ch_15_Ensemble_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRDvwyCy6aFO"
      },
      "source": [
        "# Random forest with scikit-learn on Wines Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZI35gyw52Rh"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.datasets import load_wine\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTIPjxpx6Ajx"
      },
      "source": [
        "wine = load_wine()\r\n",
        "X, Y = wine[\"data\"], wine[\"target\"]\r\n",
        "ss = StandardScaler()\r\n",
        "Xs = ss.fit_transform(X)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN6ZoIs-6KC8"
      },
      "source": [
        "lr = LogisticRegression(max_iter=5000, solver='lbfgs', multi_class='auto', random_state=1000)\r\n",
        "scores_lr = cross_val_score(lr, Xs, Y, cv=10, n_jobs=-1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYoRka786LnE"
      },
      "source": [
        "dt = DecisionTreeClassifier(criterion='entropy',\r\n",
        " max_depth=5,\r\n",
        " random_state=1000)\r\n",
        "scores_dt = cross_val_score(dt, Xs, Y, cv=10,\r\n",
        " n_jobs=-1)\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBxFb15G6O6A"
      },
      "source": [
        "svm = SVC(kernel='rbf',\r\n",
        " gamma='scale',\r\n",
        " random_state=1000)\r\n",
        "scores_svm = cross_val_score(svm, Xs, Y, cv=10,\r\n",
        " n_jobs=-1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyogxDVW6S2P",
        "outputId": "398ab0f5-55e3-4c93-cacd-8408001e7b21"
      },
      "source": [
        "print(\"Avg. Logistic Regression CV Score: {:.3f}\".\r\n",
        " format(np.mean(scores_lr)))\r\n",
        "print(\"Avg. Decision Tree CV Score: {:.3f}\".\r\n",
        " format(np.mean(scores_dt)))\r\n",
        "print(\"Avg. SVM CV Score: {:.3f}\".\r\n",
        " format(np.mean(scores_svm)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. Logistic Regression CV Score: 0.978\n",
            "Avg. Decision Tree CV Score: 0.893\n",
            "Avg. SVM CV Score: 0.978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMFexkPK7GJ_",
        "outputId": "da2541b0-5221-4c3d-ca71-b3aafebd13c3"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "rf = RandomForestClassifier(n_estimators=255,\r\n",
        " n_jobs=-1,\r\n",
        " criterion='entropy',\r\n",
        " random_state=1000)\r\n",
        "scores = cross_val_score(rf, Xs, Y, cv=10,\r\n",
        " n_jobs=-1)\r\n",
        "print(\"Avg. Random Forest CV score: {:.3f}\".\r\n",
        " format(np.mean(scores)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Avg. Random Forest CV score: 0.983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL7tHgnu6kWA"
      },
      "source": [
        "# Feature Selection using SelectFromModel from scikit-learn\r\n",
        "\r\n",
        "Given a dataset, it's important to remember that the predictive power of the features\r\n",
        "changes with the predicted value(s).\r\n",
        "In other words, the feature importance is not an intrinsic property of the dataset (like\r\n",
        "the principal components), but rather is a function of the specific task. It can happen\r\n",
        "that a large dataset containing thousands of features can be reduced to a fraction\r\n",
        "of them for particular predictions, while it could completely discard them if the\r\n",
        "goal is changed. If there are more targets to predict and each of them is associated\r\n",
        "with specific predictor sets, it can be a good idea to create a pipeline that outputs\r\n",
        "the training/validation sets for each task. This approach has a clear advantage with\r\n",
        "respect to using the whole dataset, in fact, in terms of XAI, it's much easier to show\r\n",
        "the responsibilities of the important features while discarding all those factors that\r\n",
        "don't play a primary role. Moreover, the computational cost is still higher than the\r\n",
        "space cost, therefore it's not an issue to have multiple specialized copies of the same\r\n",
        "dataset if this improves the model performances and helps the domain experts in\r\n",
        "understanding the outcomes.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u7TBEn_6Xiz",
        "outputId": "6ebc090e-41e1-4db8-b334-315f955d2af6"
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel\r\n",
        "rf.fit(X, Y)\r\n",
        "sfm = SelectFromModel(estimator=rf, prefit=True, threshold=0.02)\r\n",
        "X_sfm = sfm.transform(X)\r\n",
        "print('Feature selection shape: {}'.format(X_sfm.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature selection shape: (178, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EQQwdueWpju"
      },
      "source": [
        "# AdaBoost with scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVlACLHJ_LKG"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "scores_ne = []\r\n",
        "for ne in range(10, 201, 10):\r\n",
        "  adc = AdaBoostClassifier(n_estimators=ne,\r\n",
        "  learning_rate=0.8,\r\n",
        "  random_state=1000)\r\n",
        "  scores_ne.append(np.mean(\r\n",
        "  cross_val_score(adc, X, Y,\r\n",
        "  cv=10,\r\n",
        "  n_jobs=-1)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoXXsi31Zcli"
      },
      "source": [
        "from sklearn.decomposition import PCA, FactorAnalysis\r\n",
        "scores_pca = []\r\n",
        "for i in range(13, 1, -1):\r\n",
        "  if i < 12:\r\n",
        "    pca = PCA(n_components=i,\r\n",
        "    random_state=1000)\r\n",
        "    X_pca = pca.fit_transform(X)\r\n",
        "  else:\r\n",
        "    X_pca = X\r\n",
        "    adc = AdaBoostClassifier(n_estimators=125,\r\n",
        "    learning_rate=0.8,\r\n",
        "    random_state=1000)\r\n",
        "    scores_pca.append(np.mean(\r\n",
        "    cross_val_score(adc, X_pca, Y,\r\n",
        "    n_jobs=-1, cv=10)))\r\n",
        "    \r\n",
        "scores_fa = []\r\n",
        "for i in range(13, 1, -1):\r\n",
        "    if i < 12:\r\n",
        "      fa = FactorAnalysis(n_components=i,\r\n",
        "      random_state=1000)\r\n",
        "      X_fa = fa.fit_transform(X)\r\n",
        "    else:\r\n",
        "      X_fa = X\r\n",
        "      adc = AdaBoostClassifier(n_estimators=125,\r\n",
        "      learning_rate=0.8,\r\n",
        "      random_state=1000)\r\n",
        "      scores_fa.append(np.mean(\r\n",
        "      cross_val_score(adc, X_fa, Y,\r\n",
        "      n_jobs=-1,\r\n",
        "      cv=10)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWrnngWecLm5"
      },
      "source": [
        "This exercise confirms some important features analyzed in Chapter 13, Component\r\n",
        "Analysis and Dimensionality Reduction. First of all, performances are not dramatically\r\n",
        "affected even by a 50% dimensionality reduction. This consideration is further\r\n",
        "confirmed by the feature importance analysis performed in the previous example.\r\n",
        "Decision trees can perform quite a good classification considering only 6/7 features\r\n",
        "because the remaining ones offer a marginal contribution to the characterization of\r\n",
        "a sample. Moreover, FA is almost always superior to PCA. With 7 components, the\r\n",
        "accuracy achieved using the FA algorithm is higher than 0.95 (very close to the value\r\n",
        "achieved with no reduction), while PCA reaches this value with 12 components. The\r\n",
        "reader should remember that PCA is a particular case of FA, with the assumption\r\n",
        "of homoscedastic noise. The diagram confirms that this condition is not acceptable\r\n",
        "with the Wine dataset. Assuming different noise variances allows remodelling the\r\n",
        "reduced dataset in a more accurate way, minimizing the cross-effect of the missing\r\n",
        "features. Even if PCA is normally the first choice, with large datasets, I suggest\r\n",
        "you always compare the performance with a Factor Analysis (FA) and choose the\r\n",
        "technique that guarantees the best result (given also that FA is more expensive in\r\n",
        "terms of computational complexity)."
      ]
    }
  ]
}