{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ch-16-Advanced-Boosting-Algorithms.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxBhnfql3e0sXCVjYSHACP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekchandrajha/mastering-ml-algorithms/blob/main/Ch_16_Advanced_Boosting_Algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68iAbkp2O8-_"
      },
      "source": [
        "# Advanced Boosting Algos\r\n",
        "\r\n",
        "When working with specific classifier families (such\r\n",
        "as logistic regression or neural networks), it's very easy to include an L1\r\n",
        " or L2\r\n",
        " penalty,\r\n",
        "but it's not so easy with other estimators. For this reason, a common regularization\r\n",
        "technique (implemented also by scikit-learn) is the downsampling of the training\r\n",
        "dataset. Selecting P < N random data points allows the estimators to reduce the\r\n",
        "variance and prevent overfitting. \r\n",
        "\r\n",
        "Alternatively, it's possible to employ a random feature selection (for gradient\r\n",
        "tree boosting only) as in a random forest; picking a fraction of the total number\r\n",
        "of features increases the uncertainty and avoids over-specialization. Of course,\r\n",
        "the main drawback to these techniques is a loss of accuracy (proportional to the\r\n",
        "downsampling/feature selection ratio) that must be analyzed in order to find the\r\n",
        "most appropriate trade-off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOSPEANEWRWX"
      },
      "source": [
        "# Gradient tree boosting with scikit-learn\r\n",
        "\r\n",
        "In this example, we want to employ a gradient tree boosting classifier (class\r\n",
        "GradientBoostingClassifier) and check the impact of the maximum tree depth\r\n",
        "(parameter max_depth) on performance. Considering the previous example, we start\r\n",
        "by setting n_estimators=50 and learning_rate=0.8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zET6h6igU8Y"
      },
      "source": [
        "import numpy as np\r\n",
        "import joblib\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "scores_md = []\r\n",
        "eta = 0.8"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTdswOkwXCvf"
      },
      "source": [
        "from sklearn.datasets import load_wine\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "wine = load_wine()\r\n",
        "X, Y = wine[\"data\"], wine[\"target\"]\r\n",
        "ss = StandardScaler()\r\n",
        "Xs = ss.fit_transform(X)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeJuFTJNWisz"
      },
      "source": [
        "for md in range(2, 13):\r\n",
        "    gbc = GradientBoostingClassifier(n_estimators=50,\r\n",
        "    learning_rate=eta,\r\n",
        "    max_depth=md,\r\n",
        "    random_state=1000)\r\n",
        "    scores_md.append(np.mean(\r\n",
        "    cross_val_score(gbc, X, Y,\r\n",
        "    n_jobs=joblib.cpu_count(), cv=10)))\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AvMevmbWt9h",
        "outputId": "6316f11a-cf4a-46e9-d6f0-ec3d051b1c2a"
      },
      "source": [
        "gbc"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.8, loss='deviance', max_depth=12,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=50,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=1000, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqmwbEJnXdIq"
      },
      "source": [
        "As explained in the first section, the maximum depth of a decision tree is strictly\r\n",
        "related to the possibility of interaction among features. This can be a positive\r\n",
        "or negative aspect when the trees are employed in an ensemble. A very high\r\n",
        "interaction level can create over-complex separation hyperplanes and reduce the\r\n",
        "overall variance. In other cases, a limited interaction results in a higher bias.\r\n",
        "With this particular (and simple) dataset, the gradient boosting algorithm can\r\n",
        "achieve better performances when the max depth is two (consider that the root has\r\n",
        "a depth equal to zero) and this is partially confirmed by both the feature importance\r\n",
        "analysis and dimensionality reductions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrCyGImhXQaK"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "scores_eta = []\r\n",
        "\r\n",
        "for eta in np.linspace(0.01, 1.0, 100):\r\n",
        "    gbr = GradientBoostingClassifier(n_estimators=50,\r\n",
        "    learning_rate=eta,\r\n",
        "    max_depth=2,\r\n",
        "    random_state=1000)\r\n",
        "    scores_eta.append(\r\n",
        "    np.mean(cross_val_score(gbr, X, Y,\r\n",
        "    n_jobs=-1, cv=10)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VGiewOGYhpp",
        "outputId": "e49803c5-d7c2-4fe2-d0cd-c76ae3c835f7"
      },
      "source": [
        "scores_eta"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9163398692810457,\n",
              " 0.9218954248366013,\n",
              " 0.9218954248366013,\n",
              " 0.9277777777777778,\n",
              " 0.95,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.961111111111111,\n",
              " 0.961111111111111,\n",
              " 0.961111111111111,\n",
              " 0.961111111111111,\n",
              " 0.961111111111111,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.961111111111111,\n",
              " 0.9555555555555555,\n",
              " 0.961111111111111,\n",
              " 0.961111111111111,\n",
              " 0.9444444444444444,\n",
              " 0.9555555555555555,\n",
              " 0.95,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.95,\n",
              " 0.9444444444444444,\n",
              " 0.95,\n",
              " 0.9666666666666668,\n",
              " 0.9555555555555555,\n",
              " 0.95,\n",
              " 0.9555555555555555,\n",
              " 0.95,\n",
              " 0.961111111111111,\n",
              " 0.9555555555555555,\n",
              " 0.961111111111111,\n",
              " 0.9666666666666668,\n",
              " 0.9666666666666668,\n",
              " 0.95,\n",
              " 0.95,\n",
              " 0.9722222222222221,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.949673202614379,\n",
              " 0.9555555555555555,\n",
              " 0.9777777777777779,\n",
              " 0.9555555555555555,\n",
              " 0.9666666666666666,\n",
              " 0.9666666666666668,\n",
              " 0.9555555555555555,\n",
              " 0.9666666666666666,\n",
              " 0.95,\n",
              " 0.9722222222222221,\n",
              " 0.9722222222222221,\n",
              " 0.95,\n",
              " 0.9777777777777779,\n",
              " 0.961111111111111,\n",
              " 0.9555555555555555,\n",
              " 0.9722222222222221,\n",
              " 0.9555555555555555,\n",
              " 0.9555555555555555,\n",
              " 0.95,\n",
              " 0.949673202614379,\n",
              " 0.9722222222222221,\n",
              " 0.9777777777777779,\n",
              " 0.9663398692810456,\n",
              " 0.9722222222222221,\n",
              " 0.9777777777777779,\n",
              " 0.9722222222222221,\n",
              " 0.9777777777777779,\n",
              " 0.9722222222222221,\n",
              " 0.9777777777777779,\n",
              " 0.9552287581699346,\n",
              " 0.9663398692810456,\n",
              " 0.9444444444444444,\n",
              " 0.9555555555555555,\n",
              " 0.9666666666666666,\n",
              " 0.961111111111111,\n",
              " 0.9388888888888889,\n",
              " 0.9444444444444444,\n",
              " 0.9666666666666668,\n",
              " 0.9274509803921568,\n",
              " 0.9718954248366012,\n",
              " 0.961111111111111,\n",
              " 0.949673202614379,\n",
              " 0.949673202614379,\n",
              " 0.9607843137254901,\n",
              " 0.9722222222222221,\n",
              " 0.9722222222222221,\n",
              " 0.9441176470588235,\n",
              " 0.9833333333333334,\n",
              " 0.9552287581699346,\n",
              " 0.9666666666666668,\n",
              " 0.9444444444444444,\n",
              " 0.949673202614379]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ4cKJirgjcm"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJUjp2c1f-CR"
      },
      "source": [
        "from sklearn.datasets import load_wine\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "wine = load_wine()\r\n",
        "X, Y = wine[\"data\"], wine[\"target\"]\r\n",
        "X_train, X_test, Y_train, Y_test = \\\r\n",
        " train_test_split(X, Y,test_size=0.15, random_state=1000)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXLot7eKg20f"
      },
      "source": [
        "At this point, we need to prepare the data in a format called DMatrix, which is\r\n",
        "compatible with XGBoost. Luckily, the framework allows us to load almost any kind\r\n",
        "of data structure. Therefore, we just need to instantiate the classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49pKduaCgwqT"
      },
      "source": [
        "import xgboost as xgb\r\n",
        "dall = xgb.DMatrix(X, label=Y,\r\n",
        " feature_names=wine['feature_names'])\r\n",
        "dtrain = xgb.DMatrix(X_train, label=Y_train,\r\n",
        " feature_names=wine['feature_names'])\r\n",
        "dtest = xgb.DMatrix(X_test, label=Y_test,\r\n",
        " feature_names=wine['feature_names'])\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y51yMfphg7G4",
        "outputId": "877570bf-ac61-40cc-8d02-85244e65a1c7"
      },
      "source": [
        "dall"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<xgboost.core.DMatrix at 0x7fbab11b4710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnxI8Op-hOHw"
      },
      "source": [
        " XGBoost offers two valid alternatives for multiclass\r\n",
        "problems: Softmax and Softprob. We are employing the latter, which is often known\r\n",
        "as Softmax. In fact, the output will be a probability vector yi\r\n",
        " = (p(c = 1), p(c = 2), …p(c\r\n",
        "= m)) where each term p(c = i) represents the relative probability that the right class\r\n",
        "is i."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehe2kY2Bg9NO"
      },
      "source": [
        "import joblib\r\n",
        "params = {\r\n",
        " 'n_estimators': 50,\r\n",
        " 'max_depth': 2,\r\n",
        " 'eta': 1.0,\r\n",
        " 'objective': 'multi:softprob','eval_metric': 'mlogloss',\r\n",
        " 'num_class': 3,\r\n",
        " 'lambda': 1.0,\r\n",
        " 'seed': 1000,\r\n",
        " 'nthread': joblib.cpu_count(),\r\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y2hkJ8Lhn9E"
      },
      "source": [
        "The max depth of the trees (Nc\r\n",
        " = 50) has been set to 2 to avoid overfitting. The\r\n",
        "learning rate 𝜂𝜂 has been set to 1.0 and the parameter 𝜆𝜆, which controls the L2\r\n",
        "regularization, has been kept to its default value (1.0). This choice has been made\r\n",
        "after a simple grid search, but I invite the reader to re-implement the exercise using\r\n",
        "the XGClassifier class, which is compatible with scikit-learn and can be analyzed\r\n",
        "using GridSearchCV. It's always important to repeat that such large-capacity models,\r\n",
        "when working with small datasets, can easily overfit. This behavior would be\r\n",
        "paradoxical, because the validation accuracy could be lower than a simpler linear\r\n",
        "model. The use of L2\r\n",
        " regularization prevents the model (or, at least, mitigates the\r\n",
        "tendency) from overlearning the training set, hence its usage is always a factor to\r\n",
        "consider."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpZYSKUrhbSD",
        "outputId": "d34bf27d-fb85-425d-ea03-f90a6850bac4"
      },
      "source": [
        "nb_rounds = 20\r\n",
        "cv_model = xgb.cv(params, dall,\r\n",
        " nb_rounds,\r\n",
        " nfold=10,\r\n",
        " seed=1000)\r\n",
        "print(cv_model) #.describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    train-mlogloss-mean  ...  test-mlogloss-std\n",
            "0              0.284897  ...           0.079161\n",
            "1              0.121082  ...           0.089228\n",
            "2              0.059105  ...           0.110782\n",
            "3              0.032789  ...           0.098858\n",
            "4              0.021392  ...           0.092548\n",
            "5              0.015451  ...           0.091961\n",
            "6              0.012204  ...           0.091815\n",
            "7              0.010646  ...           0.092059\n",
            "8              0.009978  ...           0.093110\n",
            "9              0.009623  ...           0.093007\n",
            "10             0.009349  ...           0.092107\n",
            "11             0.009126  ...           0.093106\n",
            "12             0.008947  ...           0.091982\n",
            "13             0.008783  ...           0.091358\n",
            "14             0.008652  ...           0.090829\n",
            "15             0.008537  ...           0.090441\n",
            "16             0.008443  ...           0.090323\n",
            "17             0.008365  ...           0.090024\n",
            "18             0.008301  ...           0.089393\n",
            "19             0.008251  ...           0.089549\n",
            "\n",
            "[20 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9agszglqh1Jz",
        "outputId": "1244f5bc-afa1-4c22-9170-0b2afba4f5b4"
      },
      "source": [
        "evals = [(dtest, 'test'), (dtrain, 'train')]\r\n",
        "model = xgb.train(params, dtrain,\r\n",
        " nb_rounds, evals)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\ttest-mlogloss:0.458516\ttrain-mlogloss:0.278144\n",
            "[1]\ttest-mlogloss:0.287964\ttrain-mlogloss:0.113728\n",
            "[2]\ttest-mlogloss:0.224204\ttrain-mlogloss:0.051498\n",
            "[3]\ttest-mlogloss:0.180763\ttrain-mlogloss:0.029227\n",
            "[4]\ttest-mlogloss:0.145213\ttrain-mlogloss:0.018459\n",
            "[5]\ttest-mlogloss:0.140144\ttrain-mlogloss:0.01348\n",
            "[6]\ttest-mlogloss:0.13465\ttrain-mlogloss:0.010576\n",
            "[7]\ttest-mlogloss:0.143571\ttrain-mlogloss:0.009546\n",
            "[8]\ttest-mlogloss:0.14249\ttrain-mlogloss:0.009148\n",
            "[9]\ttest-mlogloss:0.139503\ttrain-mlogloss:0.008862\n",
            "[10]\ttest-mlogloss:0.138662\ttrain-mlogloss:0.00886\n",
            "[11]\ttest-mlogloss:0.138265\ttrain-mlogloss:0.00886\n",
            "[12]\ttest-mlogloss:0.138077\ttrain-mlogloss:0.00886\n",
            "[13]\ttest-mlogloss:0.137987\ttrain-mlogloss:0.00886\n",
            "[14]\ttest-mlogloss:0.137943\ttrain-mlogloss:0.00886\n",
            "[15]\ttest-mlogloss:0.137922\ttrain-mlogloss:0.00886\n",
            "[16]\ttest-mlogloss:0.137912\ttrain-mlogloss:0.00886\n",
            "[17]\ttest-mlogloss:0.137907\ttrain-mlogloss:0.00886\n",
            "[18]\ttest-mlogloss:0.137905\ttrain-mlogloss:0.00886\n",
            "[19]\ttest-mlogloss:0.137903\ttrain-mlogloss:0.00886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOD-I76eiJyS",
        "outputId": "a8863322-603a-4e3d-b8de-c8e2d42fa93f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\r\n",
        "Y_pred = model.predict(dtest)\r\n",
        "print(confusion_matrix(Y_test,\r\n",
        " np.argmax(Y_pred, axis=1)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6  0  0]\n",
            " [ 0 13  1]\n",
            " [ 0  0  7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDIkxiVbp2i2",
        "outputId": "fa21642a-2576-4b33-89d7-117f9db61ace"
      },
      "source": [
        "!pip install shap"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/20/54381999efe3000f70a7f68af79ba857cfa3f82278ab0e02e6ba1c06b002/shap-0.38.1.tar.gz (352kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 22.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 81kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92kB 8.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 245kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 276kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 327kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap) (1.1.5)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap) (4.41.1)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/c2/b3f55dfdb8af9812fdb9baf70cacf3b9e82e505b2bd4324d588888b81202/slicer-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from shap) (0.48.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from shap) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap) (2018.9)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->shap) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->shap) (51.3.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.38.1-cp36-cp36m-linux_x86_64.whl size=489404 sha256=8ed84d1b868dc3237841df378c7cc7f144b42ebc53a231cb1e6c9d2a8ec29016\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/fb/e4/88012be41842b9be62ae18d82d1b1e880daf8539d1fef1fa00\n",
            "Successfully built shap\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.38.1 slicer-0.0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3UG16Dvhses"
      },
      "source": [
        "import shap\r\n",
        "xg_explainer = shap.TreeExplainer(model)\r\n",
        "shap_values = xg_explainer.shap_values(X)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpbA10l8p1YJ",
        "outputId": "cbc63c0f-01b1-457e-a956-52542c9c7746"
      },
      "source": [
        "shap_values"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 0.5820259 ,  0.        , -0.00701695, ...,  0.        ,\n",
              "          0.        ,  2.2293222 ],\n",
              "        [ 0.43322977,  0.        , -0.00701695, ...,  0.        ,\n",
              "          0.        ,  2.3781183 ],\n",
              "        [ 0.43322977,  0.        , -0.00701695, ...,  0.        ,\n",
              "          0.        ,  2.4831321 ],\n",
              "        ...,\n",
              "        [ 0.4700444 ,  0.        , -0.00701695, ...,  0.        ,\n",
              "          0.        ,  1.0358897 ],\n",
              "        [ 0.10726569,  0.        , -0.00701695, ...,  0.        ,\n",
              "          0.        ,  0.97070324],\n",
              "        [ 0.4700444 ,  0.        , -0.01866045, ...,  0.        ,\n",
              "          0.        , -1.2874091 ]], dtype=float32),\n",
              " array([[-1.5334089 ,  0.06683041,  0.        , ...,  0.07845446,\n",
              "          0.        , -0.1029191 ],\n",
              "        [-1.5334089 , -0.15197295,  0.        , ...,  0.07845446,\n",
              "          0.        , -0.1029191 ],\n",
              "        [-1.4450097 , -0.35949442,  0.        , ..., -0.00994464,\n",
              "          0.        , -0.1029191 ],\n",
              "        ...,\n",
              "        [-1.0104495 , -0.35949442,  0.        , ..., -0.21908206,\n",
              "          0.        , -0.1029191 ],\n",
              "        [-1.0104495 , -0.35949442,  0.        , ..., -0.21908206,\n",
              "          0.        , -0.1029191 ],\n",
              "        [-1.0104495 , -0.35949442,  0.        , ..., -0.21908206,\n",
              "          0.        , -0.1029191 ]], dtype=float32),\n",
              " array([[ 0.02640481,  0.        ,  0.        , ..., -0.20422429,\n",
              "         -0.5516522 ,  0.        ],\n",
              "        [ 0.02640481,  0.        ,  0.        , ..., -0.20422429,\n",
              "         -0.5516522 ,  0.        ],\n",
              "        [ 0.02640481,  0.        ,  0.        , ..., -0.20422429,\n",
              "         -0.5516522 ,  0.        ],\n",
              "        ...,\n",
              "        [ 0.07534807,  0.        ,  0.        , ...,  0.12123479,\n",
              "          1.2469504 ,  0.        ],\n",
              "        [ 0.07534807,  0.        ,  0.        , ...,  0.12123479,\n",
              "          1.2469504 ,  0.        ],\n",
              "        [ 0.07534807,  0.        ,  0.        , ...,  0.12123479,\n",
              "          1.2469504 ,  0.        ]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EsHe2q4shYe"
      },
      "source": [
        "# Voting Classifiers\r\n",
        "\r\n",
        " As the concept is\r\n",
        "very simple, our goal is to show how to combine two completely different estimators\r\n",
        "to improve the overall cross-validation accuracy. For this reason, we have selected\r\n",
        "a logistic regression and a non-linear classifier (an RBF SVM), which are structurally\r\n",
        "different. In particular, while the former is a linear model, the latter is a kernel-based\r\n",
        "classifier that can solve complex non-linear problems.\r\n",
        "\r\n",
        "The reason why we are employing these algorithms is that we'd like to classify\r\n",
        "correctly the majority of data points using the linear model and exploit the non-linear\r\n",
        "abilities of the SVM to reduce the uncertainty associated with borderline points. As\r\n",
        "already pointed out, this dataset is quite simple and it's surprising how accurate a\r\n",
        "soft voting classifier can be compared to the complexity of other methods.\r\n",
        "This observation has to be considered from two opposite viewpoints. The first\r\n",
        "one is about the complexity of the datasets employed in the examples (which\r\n",
        "often require an ensemble). We have already explained that our goal is to show\r\n",
        "the effectiveness of the methodologies and not to apply them in real-life cases\r\n",
        "that require long training phases. Therefore, the results previously obtained are\r\n",
        "absolutely valid and show how such models can overcome the limits of simpler\r\n",
        "algorithms.\r\n",
        "On the other side, it's helpful to consider this example as an actual application of\r\n",
        "the Occam's razor principle. Sometimes, more complex models seem to perform\r\n",
        "better, but slight modifications of simpler ones can make them much more accurate\r\n",
        "and cost-effective. Considering that this is a didactic book, the reader should pay\r\n",
        "attention to this kind of compromise and learn when it makes sense to dedicate some\r\n",
        "time to optimize simpler models instead of switching to more complex (and often\r\n",
        "unmanageable) solutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsbW8u2oqBQ-"
      },
      "source": [
        "X, Y = wine[\"data\"], wine[\"target\"]\r\n",
        "ss = StandardScaler()\r\n",
        "X = ss.fit_transform(X)\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swb4KVKHtGiG",
        "outputId": "a7b3ed8a-1652-4121-cf96-85dcadf099ed"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "svm = SVC(kernel='rbf',\r\n",
        " gamma=0.01,\r\n",
        " random_state=1000)\r\n",
        "\r\n",
        "print('SCM score: {:.3f}'.format(\r\n",
        " np.mean(cross_val_score(svm, X, Y,\r\n",
        " n_jobs=-1, cv=10))))\r\n",
        "\r\n",
        "lr = LogisticRegression(C=2.0,\r\n",
        " max_iter=5000,\r\n",
        " solver='lbfgs',\r\n",
        " multi_class='auto',\r\n",
        " random_state=1000)\r\n",
        "\r\n",
        "print('Logistic Regression score: {:.3f}'.format(\r\n",
        " np.mean(cross_val_score(lr, X, Y,\r\n",
        " n_jobs=joblib.cpu_count(), cv=10))))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCM score: 0.983\n",
            "Logistic Regression score: 0.983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbfrcUiptatz"
      },
      "source": [
        "As expected, the logistic regression achieved a similar average CV accuracy as the\r\n",
        "SVM (about 98.4%). Therefore, considering the different nature of the classifiers, a\r\n",
        "hard-voting strategy is not the best choice. As we trust both classifiers and we'd like\r\n",
        "to exploit the individual features, we have chosen a soft voting with a weight vector\r\n",
        "set to (0.5, 0.5). In this way, no classifier is dominant and each of them will contribute\r\n",
        "equally to the prediction. Of course, we expect the SVM to be determinant in all\r\n",
        "those borderline cases where the linearity of the logistic regression loses the ability\r\n",
        "to capture small deviances.\r\n",
        "\r\n",
        "\r\n",
        "The class VotingClassifier accepts a list of tuples (name of the estimator, instance)\r\n",
        "that must be supplied through the estimators parameter. The strategy can be specified using parameter voting (it can be either \"soft\" or \"hard\")\r\n",
        "and the optional weights, using the parameter with the same name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJGntO0btN4a",
        "outputId": "bc01217e-4e0b-44cb-e66a-877da9b5bcab"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier\r\n",
        "\r\n",
        "vc = VotingClassifier(estimators=[\r\n",
        " ('LR', LogisticRegression(C=2.0,\r\n",
        " max_iter=5000,\r\n",
        " solver='lbfgs',\r\n",
        " multi_class='auto',\r\n",
        " random_state=1000)),\r\n",
        " ('SVM', SVC(kernel='rbf',\r\n",
        " gamma=0.01,\r\n",
        " probability=True,\r\n",
        " random_state=1000))],\r\n",
        " voting='soft',\r\n",
        " weights=(0.5, 0.5))\r\n",
        "\r\n",
        "print('Voting classifier score: {:.3f}'.format(\r\n",
        " np.mean(cross_val_score(vc, X, Y,\r\n",
        " n_jobs=-1, cv=10))))\r\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Voting classifier score: 0.994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSvvv3aHt1HA"
      },
      "source": [
        "Using a soft-voting strategy, the resulting estimator is able to outperform both the\r\n",
        "logistic regression and the SVM by reducing the global uncertainty and reaching\r\n",
        "an average CV score of about 99.4%. Indeed, the Wine dataset is almost linearly\r\n",
        "separable, but there are a few data points that lie in the region that must always be\r\n",
        "misclassified with a linear model. The presence of the RBF SVM enables this limit to\r\n",
        "be overcome and helps the logistic regression when the sigmoid value is close to 0.5.\r\n",
        "In those cases, the contribution of the SVM is enough to push the output above or\r\n",
        "below the threshold so as to obtain a precise final classification.\r\n",
        "\r\n",
        "in classical machine learning contexts, cross-validation is the only way to\r\n",
        "check the behavior of a model when trained with a large random subset and tested\r\n",
        "on the remaining subsample. Ideally, we'd like to observe the same performances,\r\n",
        "but it can also happen that the accuracy is higher in some folds and quite a bit\r\n",
        "lower in others. When this phenomenon is observed and the dataset is the final\r\n",
        "one, it probably means that the model is not able to manage one or more regions\r\n",
        "of the sample space and a boosting approach could dramatically improve the final\r\n",
        "accuracy."
      ]
    }
  ]
}