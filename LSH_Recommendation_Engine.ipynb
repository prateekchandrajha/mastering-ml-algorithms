{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSH-Recommendation-Engine.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMTwGpWbTvFdYgLGUInxwVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prateekchandrajha/mastering-ml-algorithms/blob/main/LSH_Recommendation_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBDctErHum4n"
      },
      "source": [
        "Read: https://www.learndatasci.com/tutorials/building-recommendation-engine-locality-sensitive-hashing-lsh-python/\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee0xVwjMurvy",
        "outputId": "7b57c84f-bd59-4a5d-c678-d19a32f95435"
      },
      "source": [
        "!pip install datasketch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasketch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/35/3e39356d97dc67c4bddaddb51693c20a6eb61e535ce5be09d3755ba2b823/datasketch-1.5.3-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 17.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 19.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 40kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from datasketch) (1.19.5)\n",
            "Installing collected packages: datasketch\n",
            "Successfully installed datasketch-1.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LhXnwD4ubK8"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import time\r\n",
        "from datasketch import MinHash, MinHashLSHForest"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wijWzqVu1oE"
      },
      "source": [
        "#Preprocess will split a string of text into individual tokens/shingles based on whitespace.\r\n",
        "def preprocess(text):\r\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\r\n",
        "    tokens = text.lower()\r\n",
        "    tokens = tokens.split()\r\n",
        "    return tokens"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TSel8FNvDyC",
        "outputId": "1915ad2b-0c07-4113-f127-a4ed773cf3fa"
      },
      "source": [
        "text = 'The devil went down to Georgia'\r\n",
        "print('The shingles (tokens) are:', preprocess(text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shingles (tokens) are: ['the', 'devil', 'went', 'down', 'to', 'georgia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZbZA7Q3vGkA"
      },
      "source": [
        "#Number of Permutations\r\n",
        "permutations = 128\r\n",
        "\r\n",
        "#Number of Recommendations to return\r\n",
        "num_recommendations = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9udNrXQjvRqt"
      },
      "source": [
        "In order to create the Minhash Forest, we will execute the following steps:\r\n",
        "\r\n",
        "Pass in a dataframe with every string you want to query.\r\n",
        "Preprocess a string of text using our preprocessing step above.\r\n",
        "Set the number of permutations in your MinHash.\r\n",
        "MinHash the string on all of your shingles in the string.\r\n",
        "Store the MinHash of the string.\r\n",
        "Repeat 2-5 for all strings in your dataframe.\r\n",
        "Build a forest of all the MinHashed strings.\r\n",
        "Index your forest to make it searchable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvxZhI74vJxe"
      },
      "source": [
        "def get_forest(data, perms):\r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    minhash = []\r\n",
        "    \r\n",
        "    for text in data['text']:\r\n",
        "        tokens = preprocess(text)\r\n",
        "        m = MinHash(num_perm=perms)\r\n",
        "        for s in tokens:\r\n",
        "            m.update(s.encode('utf8'))\r\n",
        "        minhash.append(m)\r\n",
        "        \r\n",
        "    forest = MinHashLSHForest(num_perm=perms)\r\n",
        "    \r\n",
        "    for i,m in enumerate(minhash):\r\n",
        "        forest.add(i,m)\r\n",
        "        \r\n",
        "    forest.index()\r\n",
        "    \r\n",
        "    print('It took %s seconds to build forest.' %(time.time()-start_time))\r\n",
        "    \r\n",
        "    return forest"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs7blWtXvk23"
      },
      "source": [
        "In order to query the forest that was built, we will follow the steps below:\r\n",
        "\r\n",
        "Preprocess your text into shingles.\r\n",
        "Set the same number of permutations for your MinHash as was used to build the forest.\r\n",
        "Create your MinHash on the text using all your shingles.\r\n",
        "Query the forest with your MinHash and return the number of requested recommendations.\r\n",
        "Provide the titles of each conference paper recommended."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS1eK8_tvWau"
      },
      "source": [
        "def predict(text, database, perms, num_results, forest):\r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    tokens = preprocess(text)\r\n",
        "    m = MinHash(num_perm=perms)\r\n",
        "    for s in tokens:\r\n",
        "        m.update(s.encode('utf8'))\r\n",
        "        \r\n",
        "    idx_array = np.array(forest.query(m, num_results))\r\n",
        "    if len(idx_array) == 0:\r\n",
        "        return None # if your query is empty, return none\r\n",
        "    \r\n",
        "    result = database.iloc[idx_array]['title']\r\n",
        "    \r\n",
        "    print('It took %s seconds to query forest.' %(time.time()-start_time))\r\n",
        "    \r\n",
        "    return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRDqEgX5vqbi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR6DFtrhuk-L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}